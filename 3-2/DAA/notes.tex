\documentclass[12pt,letterpaper]{article}

%%%%%%%%%%%%
% Includes %
%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage[margin=1 in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[justification=centering]{caption}
\usepackage[ruled,vlined]{algorithm2e}

% Formatting
\renewcommand{\baselinestretch}{1.25}

% Theorems and other necessary structures
\newtheorem{definition}{Definition}[section] % Definition
\newtheorem{theorem}{Theorem}[section] % Big result
\newtheorem{corollary}{Corollary}[theorem] % Follows from a theorem
\newtheorem{lemma}[theorem]{Lemma} % Minor result

% New commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

% Title information
\title{Design and Analysis of Algorithms}
\author{2018A7PS0193P}

\begin{document}

\maketitle

\section{Fundamentals}

\begin{definition}
  An algorithm is a well defined computational procedure. It takes an input, does some computation and terminates with output
\end{definition}

To check the correctness of an algorithm, we must check the following characteristics:

\begin{itemize}
  \item Initialization: The algorithm is correct at the beginning
  \item Maintenance : The algorithm remains correct as it runs
  \item Termination : The algorithm terminates in finite time, correctly
\end{itemize}

For this entire course, we must always prove these characteristics when defining any algorithm.

Algorithms are generally defined by a complexity - the time taken to complete the computation on a given input size. There are three ways we could consider this - best case, worst case, or average case.

Complexity is discussed a lot in DSA, so I'm not going to rewrite it here. A quick roundup is:

\begin{itemize}
  \item $O(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq f(n) \leq c \cdot g(n) \forall n \geq n_0 \}$
  \item $\Omega(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq c \cdot g(n)\leq f(n) \forall n \geq n_0 \}$
  \item $\Theta(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq c_2 \cdot g(n)\leq f(n) \leq c_2 \cdot g(n) \forall n \geq n_0 \}$
\end{itemize}

To find complexities in the case of recurrences, we use the \textbf{master method}. Let the recurrence be given by:

\[T(n) = aT\left(\frac{n}{b}\right) + f(n)\]

Here, $a, b \geq 1$. Let $\epsilon$ be a constant. Then:

\begin{enumerate}
  \item If $f(n) = O(n^{\log_ba - \epsilon})$ then $T(n) = \Theta(n^{\log_ba})$
  \item If $f(n) = O(n^{\log_ba})$ then $T(n) = \Theta(n^{\log_ba}\log{n})$
  \item If $f(n) = O(n^{\log_ba + \epsilon})$ then $T(n) = \Theta(f(n))$ provided if $af(n/b) \leq cf(n)$ for some constant $c <1$ and all sufficiently large $n$.
\end{enumerate}

Here, we redo DSA despite it being a prerequisite of the course. This recap has lasted 3 lectures (so far). You should probably just read CLRS, this is a waste. The topics covered are:

\begin{itemize}
  \item Quicksort (and it's average case analysis)
  \item The $\Omega(n\log{n})$ lower bound of comparison sorting
  \item Non-comparison sorting like counting sort, radix sort, etc.
  \item Average case analysis of bucket sort
\end{itemize}

\section{Matrix Multiplication}

Naive Matrix multiplication is $\Theta(N^3)$, since we can express the result $A \cdot B  = C$ as:

\[C_{ij} = \sum_{k=1}^{r}A_{ik} \times B_{kj}\]

We can improve this using a divide-and-conquer approach with \textbf{Strassen's Multiplication}. It has four steps:

\begin{enumerate}
  \item Divide the input matrices $A$ and $B$ and the output matrix $C$ into four $n/2 \times n/2$ submatrices. This takes $\Theta(1)$ time.
  \item Create 10 matrices $S_1,S_2,...S_10$, each of which is of size $n/2 \times n/2$ and is the sum or difference of two matrices created in step 1.
  \item Using these submatrices, we can recursively compute seven matrix products $P_1,P_2,...P_7$, each of which is $n/2$.
  \item Compute the desired submatrices $C_{11},C_{12},C_{21},C_{22}$ by adding and subtracting various combinations of the $P_i$ matrices. We can compute all four in $\Theta(N^2)$ time.
\end{enumerate}

The details of this can be seen on page 80 of CLRS, but the running time recurrence will be given by:

\[T(n) = \begin{cases}
  \Theta(1) & \text{if } n = 1 \\
  7T(n/2) + \Theta(n^2) & \text{if } n > 1
\end{cases}\]

By master method, this is $T(n) = \Theta(n^{\log7})$

\section{Polynomial Multiplication}

Polynomials are functions of the form:

\[f(x) = a_0 + a_1x + a_x^2 + ... a_{n-1}x^{n-1}\]

One way to express this is as a vector of coefficients  - this is called the \textbf{Coefficient form}. This form also allows us to evaluate $f(x)$ in $O(n)$ using Horner's rule, where we express the polynomial as:

\[a_0 + x(a_1 + x(a_2 + ...x(a_n-1)...))\]

Another way to express this is using the \textbf{point value form} , where we express it as $n$ point of the form $(x_i,f(x_i))$. This point value form uniquely identifies a polynomial. Generally this would take $\Theta(N^2)$ time , but with FFT we can do it in $O(N\log N)$.

The process of getting the coefficient form from the point value form is known as $\textbf{interpolation}$.  We can do this in $O(n^3)$ using Gaussian Elimination, or in $O(n^2)$ with Lagrange Interpolation.

Generally, the multiplication of polynomials takes $\Theta(N^2)$. However, we can do this much faster using \textbf{Fast Fourier Transform}.

\subsection{Fourier Transform}

\subsubsection{Discrete Fourier Transform}

From now on, $w_n^k$ will denote the $k^{th}$ solution of $x^n=1$, i.e. the $n^{th}$ root of unity. $w_n$ will denote the principal $n^{th}$ root of unity. Remember the following properties:

\begin{enumerate}
  \item $w_n^k = e^{\frac{2k\pi i}{n}}$
  \item $w_{dn}^{dk} = w_{n}^k$
  \item $w_{n}^{n/2} = w_2 = -1$
  \item If $n >0$ is even, then the squares of the $n$ complex $n^{th}$ roots of unity are the $n/2$ complex $n/2$th roots of unity. (Halving Lemma)
  \item $\sum_{j=0}^{n-1} (w_{n}^{k})^j = 0$ (Summation Property)
\end{enumerate}

We call the vector $y = (y_0,y_1,...y_{n-1})$ the \textbf{discrete Fourier Transform}  of the polynomial $A$ if $y_k = A(w_n^k)$.

\subsubsection{Fast Fourier Transform}

FFT consists of three parts:

\begin{enumerate}
  \item Evaluation, where we find the DFT in $O(n\log n)$
  \item Pointwise Multiplication of the two DFTs
  \item Interpolation or the inverse FFT, where we find the coefficient form in $O(n \log n)$.
\end{enumerate}

Consider the following polynomials:

\[A(x) = a_0x^0 + a_1x^1 + ... + a_{n-1}x^{n-1}\]
\[A_0(x) = a_0x^0 + a_2x^1 + ... + a_{n-2}x^{n/2-1}\]
\[A_1(x) = a_1x^0 + a_3x^1 + ... + a_{n-1}x^{n/2-1}\]

It is easy to see that:

\[A(x) = A_0(x^2) + xA_1(x^2)\]

These polynomials have only half as many coefficients as the polynomial $A$. So, if we can compute $DFT(A)$ from $DFT(A_1)$ and $DFT(A_0)$ in linear time, we would be able to do this in $O(n \log n)$ (direct from master method).

We find that do this with the equations:

\[y_k = y_k^0 + w_n^k y_k^1\]
\[y_{k+n/2} = y_k^0 - w_n^k y_k^1\]

Here, $k \in [0,n/2-1]$. The proof of this can be seen on cp-algorithms.

As such, we have found the DFT of the polynomial in $O(n \log n)$ time.

After performing the pointwise multiplication of the DFT of our polynomials $A$ and $B$, we have to interpolate to find the coefficient form of our resulting polynomial $C$.

\textit{TODO: Add interpolation with Vandermonde matrix, can be seen on cp-algorithms} 

\section{Greedy algorithms}

Greedy algorithms involve making a sequence of choices where each looks best at the moment. It is making the locally optimal choice in the hope that it leads to a globally optimal solution. However, this may not always be the case. For this to work, we need the following properties:

\begin{itemize}
  \item \textbf{Greedy choice property:} we can assemble a globally optimal solution by making locally optimal (greedy) choices.
  \item \textbf{Optimal Substructure} : A problem is said to have optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. 
\end{itemize}

\subsection{Activity Selection Problem}

Consider a set $S = \{1,2,3,..n\}$ of $n$ activities that can happen one activity at a time. Activity $i$ takes place during interval $[s_i,f_i)$. Activities $i$ and $j$ are compatible if $[s_i,f_i)$ and $[s_j,f_j)$ don't overlap. Our goal is to select the maximum size subset of mutually comparable activities. 

To solve, we can assume that activities are in increasing order of their finishing time. If not, then sort it in $O(n \log n)$. Doing this, we can choose them greedily, picking an activity whenever we are free.

\subsection{Fractional Knapsack Problem}

A thief robbing a store finds $n$ items. The $i^{th}$  item is worth $v_i$ dollars and weighs $w_i$ pounds. The thief wants to take as valuable a load as possible, but he can carry at most $W$ pounds in his knapsack. Which items should he take?

If the thief can carry fractions of items, he can solve it greedily - this is called the fractional knapsack problem. Otherwise, if he can either take or leave an item (the 0-1 knapsack problem), then it needs to be solved by dynamic programming.

In the fractional knapsack problem, we can greedily choose the items with the largest value to weight ratio.

\subsection{Huffman Coding}

Huffman coding is a greedy algorithm that constructs an optimal prefix code.

Say we are given a text, along with the frequencies of each character in the text. Obviously, we want the most frequent character to take up the minimum number of bits, to minimize the total size. We can make the same arguments for the other characters in decreasing order of frequencies. For instance, if the character `a' has the most frequency, we may represent it by the bit 0, and the character `x' (which is next in the order of frequency) as 10 and so on. We have to design these so that there is no ambiguity when decoding the Huffman code. This means no code can be the prefix of any other code!

We can represent this encoding as a binary tree where going left corresponds to adding the character `0' to the code, and moving right corresponds to adding the character `1' to the code. Each character is a leaf in this binary tree, and they will definitely not be prefixes of one another.

The algorithm for Huffman Coding creates this tree. Say we are given a set $C$ of characters, along with theire frequencies. The algorithm is as follows:

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{A prefix tree for Huffman Codes}
  $n = |C|$ \\
  $Q = C$ \\
  \For{$i \gets1$ \KwTo $n-1$} {
    Allocate new node $z$ \\
    $z.left = x =$ EXTRACT\_MIN(Q)$ \\
    $z.right = y =$ EXTRACT\_MIN(Q)$ \\
    $z.freq = x.freq+y.freq$ \\ 
    INSERT($Q.z$) \\
  }
  return EXTRACT\_MIN(Q)
 \caption{Huffman Coding}
\end{algorithm}

If we use a heap, we can do this in $O(n \log n)$. It can actually be faster using van Emde Boas Tree, which would make it $O(n \log \log n)$

\end{document}
