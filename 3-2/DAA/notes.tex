\documentclass[12pt,letterpaper]{article}

%%%%%%%%%%%%
% Includes %
%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage[margin=1 in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[justification=centering]{caption}

% Formatting
\renewcommand{\baselinestretch}{1.25}

% Theorems and other necessary structures
\newtheorem{definition}{Definition}[section] % Definition
\newtheorem{theorem}{Theorem}[section] % Big result
\newtheorem{corollary}{Corollary}[theorem] % Follows from a theorem
\newtheorem{lemma}[theorem]{Lemma} % Minor result

% New commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

% Title information
\title{Design and Analysis of Algorithms}
\author{2018A7PS0193P}

\begin{document}

\maketitle

\section{Fundamentals}

\begin{definition}
  An algorithm is a well defined computational procedure. It takes an input, does some computation and terminates with output
\end{definition}

To check the correctness of an algorithm, we must check the following characteristics:

\begin{itemize}
  \item Initialization: The algorithm is correct at the beginning
  \item Maintenance : The algorithm remains correct as it runs
  \item Termination : The algorithm terminates in finite time, correctly
\end{itemize}

For this entire course, we must always prove these characteristics when defining any algorithm.

Algorithms are generally defined by a complexity - the time taken to complete the computation on a given input size. There are three ways we could consider this - best case, worst case, or average case.

Complexity is discussed a lot in DSA, so I'm not going to rewrite it here. A quick roundup is:

\begin{itemize}
  \item $O(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq f(n) \leq c \cdot g(n) \forall n \geq n_0 \}$
  \item $\Omega(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq c \cdot g(n)\leq f(n) \forall n \geq n_0 \}$
  \item $\Theta(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq c_2 \cdot g(n)\leq f(n) \leq c_2 \cdot g(n) \forall n \geq n_0 \}$
\end{itemize}

To find complexities in the case of recurrences, we use the \textbf{master method}. Let the recurrence be given by:

\[T(n) = aT\left(\frac{n}{b}\right) + f(n)\]

Here, $a, b \geq 1$. Let $\epsilon$ be a constant. Then:

\begin{enumerate}
  \item If $f(n) = O(n^{\log_ba - \epsilon})$ then $T(n) = \Theta(n^{\log_ba})$
  \item If $f(n) = O(n^{\log_ba})$ then $T(n) = \Theta(n^{\log_ba}\log{n})$
  \item If $f(n) = O(n^{\log_ba + \epsilon})$ then $T(n) = \Theta(f(n))$ provided if $af(n/b) \leq cf(n)$ for some constant $c <1$ and all sufficiently large $n$.
\end{enumerate}

Here, we redo DSA despite it being a prerequisite of the course. This recap has lasted 3 lectures (so far). You should probably just read CLRS, this is a waste. The topics covered are:

\begin{itemize}
  \item Quicksort (and it's average case analysis)
  \item The $\Omega(n\log{n})$ lower bound of comparison sorting
  \item Non-comparison sorting like counting sort, radix sort, etc.
  \item Average case analysis of bucket sort
\end{itemize}

\section{Matrix Multiplication}

Naive Matrix multiplication is $\Theta(N^3)$, since we can express the result $A \cdot B  = C$ as:

\[C_{ij} = \sum_{k=1}^{r}A_{ik} \times B_{kj}\]

We can improve this using a divide-and-conquer approach with \textbf{Strassen's Multiplication}. It has four steps:

\begin{enumerate}
  \item Divide the input matrices $A$ and $B$ and the output matrix $C$ into four $n/2 \times n/2$ submatrices. This takes $\Theta(1)$ time.
  \item Create 10 matrices $S_1,S_2,...S_10$, each of which is of size $n/2 \times n/2$ and is the sum or difference of two matrices created in step 1.
  \item Using these submatrices, we can recursively compute seven matrix products $P_1,P_2,...P_7$, each of which is $n/2$.
  \item Compute the desired submatrices $C_{11},C_{12},C_{21},C_{22}$ by adding and subtracting various combinations of the $P_i$ matrices. We can compute all four in $\Theta(N^2)$ time.
\end{enumerate}

The details of this can be seen on page 80 of CLRS, but the running time recurrence will be given by:

\[T(n) = \begin{cases}
  \Theta(1) & \text{if } n = 1 \\
  7T(n/2) + \Theta(n^2) & \text{if } n > 1
\end{cases}\]

By master method, this is $T(n) = \Theta(n^{\log7})$

\section{Polynomial Multiplication}

Polynomials are functions of the form:

\[f(x) = a_0 + a_1x + a_x^2 + ... a_{n-1}x^{n-1}\]

One way to express this is as a vector of coefficients  - this is called the \textbf{Coefficient form}. This form also allows us to evaluate $f(x)$ in $O(n)$ using Horner's rule, where we express the polynomial as:

\[a_0 + x(a_1 + x(a_2 + ...x(a_n-1)...))\]

Another way to express this is using the \textbf{point value form} , where we express it as $n$ point of the form $(x_i,f(x_i))$. This point value form uniquely identifies a polynomial. Generally this would take $\Theta(N^2)$ time , but with FFT we can do it in $O(N\log N)$.

The process of getting the coefficient form from the point value form is known as $\textbf{interpolation}$.  We can do this in $O(n^3)$ using Gaussian Elimination, or in $O(n^2)$ with Lagrange Interpolation.

Generally, the multiplication of polynomials takes $\Theta(N^2)$. However, we can do this much faster using \textbf{Fast Fourier Transform}.

\subsection{Fourier Transform}

\subsubsection{Discrete Fourier Transform}

From now on, $w_n^k$ will denote the $k^{th}$ solution of $x^n=1$, i.e. the $n^{th}$ root of unity. $w_n$ will denote the principal $n^{th}$ root of unity. Remember the following properties:

\begin{enumerate}
  \item $w_n^k = e^{\frac{2k\pi i}{n}}$
  \item $w_{dn}^{dk} = w_{n}^k$
  \item $w_{n}^{n/2} = w_2 = -1$
  \item If $n >0$ is even, then the squares of the $n$ complex $n^{th}$ roots of unity are the $n/2$ complex $n/2$th roots of unity. (Halving Lemma)
  \item $\sum_{j=0}^{n-1} (w_{n}^{k})^j = 0$ (Summation Property)
\end{enumerate}

We call the vector $y = (y_0,y_1,...y_{n-1})$ the \textbf{discrete Fourier Transform}  of the polynomial $A$ if $y_k = A(w_n^k)$.

\subsubsection{Fast Fourier Transform}

FFT consists of three parts:

\begin{enumerate}
  \item Evaluation, where we find the DFT in $O(n\log n)$
  \item Pointwise Multiplication of the two DFTs
  \item Interpolation or the inverse FFT, where we find the coefficient form in $O(n \log n)$.
\end{enumerate}

Consider the following polynomials:

\[A(x) = a_0x^0 + a_1x^1 + ... + a_{n-1}x^{n-1}\]
\[A_0(x) = a_0x^0 + a_2x^1 + ... + a_{n-2}x^{n/2-1}\]
\[A_1(x) = a_1x^0 + a_3x^1 + ... + a_{n-1}x^{n/2-1}\]

It is easy to see that:

\[A(x) = A_0(x^2) + xA_1(x^2)\]

These polynomials have only half as many coefficients as the polynomial $A$. So, if we can compute $DFT(A)$ from $DFT(A_1)$ and $DFT(A_0)$ in linear time, we would be able to do this in $O(n \log n)$ (direct from master method).

We find that do this with the equations:

\[y_k = y_k^0 + w_n^k y_k^1\]
\[y_{k+n/2} = y_k^0 - w_n^k y_k^1\]

Here, $k \in [0,n/2-1]$. The proof of this can be seen on cp-algorithms.

As such, we have found the DFT of the polynomial in $O(n \log n)$ time.

After performing the pointwise multiplication of the DFT of our polynomials $A$ and $B$, we have to interpolate to find the coefficient form of our resulting polynomial $C$.

\textit{TODO: Add interpolation with Vandermonde matrix, can be seen on cp-algorithms} 

\end{document}
