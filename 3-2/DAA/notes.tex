\documentclass[12pt,letterpaper]{article}

%%%%%%%%%%%%
% Includes %
%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
\usepackage[margin=1 in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage[justification=centering]{caption}
\usepackage[ruled,vlined]{algorithm2e}

% Formatting
\renewcommand{\baselinestretch}{1.25}

% Theorems and other necessary structures
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section] % Definition
\newtheorem{theorem}{Theorem}[section] % Big result
\newtheorem{corollary}{Corollary}[theorem] % Follows from a theorem
\newtheorem{lemma}[theorem]{Lemma} % Minor result

% New commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

% Title information
\title{Design and Analysis of Algorithms}
\author{2018A7PS0193P}

\begin{document}

\maketitle

\section{Fundamentals}

\begin{definition}
  An algorithm is a well defined computational procedure. It takes an input, does some computation and terminates with output
\end{definition}

To check the correctness of an algorithm, we must check the following characteristics:

\begin{itemize}
  \item Initialization: The algorithm is correct at the beginning
  \item Maintenance : The algorithm remains correct as it runs
  \item Termination : The algorithm terminates in finite time, correctly
\end{itemize}

For this entire course, we must always prove these characteristics when defining any algorithm.

Algorithms are generally defined by a complexity - the time taken to complete the computation on a given input size. There are three ways we could consider this - best case, worst case, or average case.

Complexity is discussed a lot in DSA, so I'm not going to rewrite it here. A quick roundup is:

\begin{itemize}
  \item $O(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq f(n) \leq c \cdot g(n) \forall n \geq n_0 \}$
  \item $\Omega(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq c \cdot g(n)\leq f(n) \forall n \geq n_0 \}$
  \item $\Theta(g(n)) = \{f(n) : \text{there exists } c, n_0 : 0 \leq c_2 \cdot g(n)\leq f(n) \leq c_2 \cdot g(n) \forall n \geq n_0 \}$
\end{itemize}

To find complexities in the case of recurrences, we use the \textbf{master method}. Let the recurrence be given by:

\[T(n) = aT\left(\frac{n}{b}\right) + f(n)\]

Here, $a, b \geq 1$. Let $\epsilon$ be a constant. Then:

\begin{enumerate}
  \item If $f(n) = O(n^{\log_ba - \epsilon})$ then $T(n) = \Theta(n^{\log_ba})$
  \item If $f(n) = O(n^{\log_ba})$ then $T(n) = \Theta(n^{\log_ba}\log{n})$
  \item If $f(n) = O(n^{\log_ba + \epsilon})$ then $T(n) = \Theta(f(n))$ provided if $af(n/b) \leq cf(n)$ for some constant $c <1$ and all sufficiently large $n$.
\end{enumerate}

Here, we redo DSA despite it being a prerequisite of the course. This recap has lasted 3 lectures (so far). You should probably just read CLRS, this is a waste. The topics covered are:

\begin{itemize}
  \item Quicksort (and it's average case analysis)
  \item The $\Omega(n\log{n})$ lower bound of comparison sorting
  \item Non-comparison sorting like counting sort, radix sort, etc.
  \item Average case analysis of bucket sort
\end{itemize}

\section{Matrix Multiplication}

Naive Matrix multiplication is $\Theta(N^3)$, since we can express the result $A \cdot B  = C$ as:

\[C_{ij} = \sum_{k=1}^{r}A_{ik} \times B_{kj}\]

We can improve this using a divide-and-conquer approach with \textbf{Strassen's Multiplication}. It has four steps:

\begin{enumerate}
  \item Divide the input matrices $A$ and $B$ and the output matrix $C$ into four $n/2 \times n/2$ submatrices. This takes $\Theta(1)$ time.
  \item Create 10 matrices $S_1,S_2,...S_10$, each of which is of size $n/2 \times n/2$ and is the sum or difference of two matrices created in step 1.
  \item Using these submatrices, we can recursively compute seven matrix products $P_1,P_2,...P_7$, each of which is $n/2$.
  \item Compute the desired submatrices $C_{11},C_{12},C_{21},C_{22}$ by adding and subtracting various combinations of the $P_i$ matrices. We can compute all four in $\Theta(N^2)$ time.
\end{enumerate}

The details of this can be seen on page 80 of CLRS, but the running time recurrence will be given by:

\[T(n) = \begin{cases}
  \Theta(1) & \text{if } n = 1 \\
  7T(n/2) + \Theta(n^2) & \text{if } n > 1
\end{cases}\]

By master method, this is $T(n) = \Theta(n^{\log7})$

\section{Polynomial Multiplication}

Polynomials are functions of the form:

\[f(x) = a_0 + a_1x + a_x^2 + ... a_{n-1}x^{n-1}\]

One way to express this is as a vector of coefficients  - this is called the \textbf{Coefficient form}. This form also allows us to evaluate $f(x)$ in $O(n)$ using Horner's rule, where we express the polynomial as:

\[a_0 + x(a_1 + x(a_2 + ...x(a_n-1)...))\]

Another way to express this is using the \textbf{point value form} , where we express it as $n$ point of the form $(x_i,f(x_i))$. This point value form uniquely identifies a polynomial. Generally this would take $\Theta(N^2)$ time , but with FFT we can do it in $O(N\log N)$.

The process of getting the coefficient form from the point value form is known as $\textbf{interpolation}$.  We can do this in $O(n^3)$ using Gaussian Elimination, or in $O(n^2)$ with Lagrange Interpolation.

Generally, the multiplication of polynomials takes $\Theta(N^2)$. However, we can do this much faster using \textbf{Fast Fourier Transform}.

\subsection{Fourier Transform}

\subsubsection{Discrete Fourier Transform}

From now on, $w_n^k$ will denote the $k^{th}$ solution of $x^n=1$, i.e. the $n^{th}$ root of unity. $w_n$ will denote the principal $n^{th}$ root of unity. Remember the following properties:

\begin{enumerate}
  \item $w_n^k = e^{\frac{2k\pi i}{n}}$
  \item $w_{dn}^{dk} = w_{n}^k$
  \item $w_{n}^{n/2} = w_2 = -1$
  \item If $n >0$ is even, then the squares of the $n$ complex $n^{th}$ roots of unity are the $n/2$ complex $n/2$th roots of unity. (Halving Lemma)
  \item $\sum_{j=0}^{n-1} (w_{n}^{k})^j = 0$ (Summation Property)
\end{enumerate}

We call the vector $y = (y_0,y_1,...y_{n-1})$ the \textbf{discrete Fourier Transform}  of the polynomial $A$ if $y_k = A(w_n^k)$.

\subsubsection{Fast Fourier Transform}

FFT consists of three parts:

\begin{enumerate}
  \item Evaluation, where we find the DFT in $O(n\log n)$
  \item Pointwise Multiplication of the two DFTs
  \item Interpolation or the inverse FFT, where we find the coefficient form in $O(n \log n)$.
\end{enumerate}

Consider the following polynomials:

\[A(x) = a_0x^0 + a_1x^1 + ... + a_{n-1}x^{n-1}\]
\[A_0(x) = a_0x^0 + a_2x^1 + ... + a_{n-2}x^{n/2-1}\]
\[A_1(x) = a_1x^0 + a_3x^1 + ... + a_{n-1}x^{n/2-1}\]

It is easy to see that:

\[A(x) = A_0(x^2) + xA_1(x^2)\]

These polynomials have only half as many coefficients as the polynomial $A$. So, if we can compute $DFT(A)$ from $DFT(A_1)$ and $DFT(A_0)$ in linear time, we would be able to do this in $O(n \log n)$ (direct from master method).

We find that do this with the equations:

\[y_k = y_k^0 + w_n^k y_k^1\]
\[y_{k+n/2} = y_k^0 - w_n^k y_k^1\]

Here, $k \in [0,n/2-1]$. The proof of this can be seen on cp-algorithms.

As such, we have found the DFT of the polynomial in $O(n \log n)$ time.

After performing the pointwise multiplication of the DFT of our polynomials $A$ and $B$, we have to interpolate to find the coefficient form of our resulting polynomial $C$.

\textit{TODO: Add interpolation with Vandermonde matrix, can be seen on cp-algorithms} 

\section{Greedy algorithms}

Greedy algorithms involve making a sequence of choices where each looks best at the moment. It is making the locally optimal choice in the hope that it leads to a globally optimal solution. However, this may not always be the case. For this to work, we need the following properties:

\begin{itemize}
  \item \textbf{Greedy choice property:} we can assemble a globally optimal solution by making locally optimal (greedy) choices.
  \item \textbf{Optimal Substructure} : A problem is said to have optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. 
\end{itemize}

\subsection{Activity Selection Problem}

Consider a set $S = \{1,2,3,..n\}$ of $n$ activities that can happen one activity at a time. Activity $i$ takes place during interval $[s_i,f_i)$. Activities $i$ and $j$ are compatible if $[s_i,f_i)$ and $[s_j,f_j)$ don't overlap. Our goal is to select the maximum size subset of mutually comparable activities. 

To solve, we can assume that activities are in increasing order of their finishing time. If not, then sort it in $O(n \log n)$. Doing this, we can choose them greedily, picking an activity whenever we are free.

\subsection{Fractional Knapsack Problem}

A thief robbing a store finds $n$ items. The $i^{th}$  item is worth $v_i$ dollars and weighs $w_i$ pounds. The thief wants to take as valuable a load as possible, but he can carry at most $W$ pounds in his knapsack. Which items should he take?

If the thief can carry fractions of items, he can solve it greedily - this is called the fractional knapsack problem. Otherwise, if he can either take or leave an item (the 0-1 knapsack problem), then it needs to be solved by dynamic programming.

In the fractional knapsack problem, we can greedily choose the items with the largest value to weight ratio.

\subsection{Huffman Coding}

Huffman coding is a greedy algorithm that constructs an optimal prefix code.

Say we are given a text, along with the frequencies of each character in the text. Obviously, we want the most frequent character to take up the minimum number of bits, to minimize the total size. We can make the same arguments for the other characters in decreasing order of frequencies. For instance, if the character `a' has the most frequency, we may represent it by the bit 0, and the character `x' (which is next in the order of frequency) as 10 and so on. We have to design these so that there is no ambiguity when decoding the Huffman code. This means no code can be the prefix of any other code!

We can represent this encoding as a binary tree where going left corresponds to adding the character `0' to the code, and moving right corresponds to adding the character `1' to the code. Each character is a leaf in this binary tree, and they will definitely not be prefixes of one another.

The algorithm for Huffman Coding creates this tree. Say we are given a set $C$ of characters, along with theire frequencies. The algorithm is as follows:

\begin{algorithm}[H]
  \SetAlgoLined
  \KwResult{A prefix tree for Huffman Codes}
  $n = |C|$ \\
  $Q = C$ \\
  \For{$i \gets1$ \KwTo $n-1$} {
    Allocate new node $z$ \\
    $z.left = x =$ EXTRACT\_MIN(Q)$ \\
    $z.right = y =$ EXTRACT\_MIN(Q)$ \\
    $z.freq = x.freq+y.freq$ \\ 
    INSERT($Q.z$) \\
  }
  return EXTRACT\_MIN(Q)
  \caption{Huffman Coding}
\end{algorithm}

If we use a heap, we can do this in $O(n \log n)$. It can actually be faster using van Emde Boas Tree, which would make it $O(n \log \log n)$

\section{Matroids}

A \textbf{matroid}  is an ordered pair $M = (S,I)$ satisfying the following conditions:

\begin{itemize}
  \item $S$ is a finite set
  \item $I$ is a non empty family of subsets of $S$ called the independent subsets of $S$, such that if $B \in I$ and $A \subseteq B$, then $A \in I$. This is the Hereditary Property.
  \item If $A \in I$, $B \in I$, and $|A| \leq |B|$ then there exists some element $x\in B-A$ such that $A \cup \{x\} \in I$. This is the exchange property.
\end{itemize}

The graphic matroid $M_G = (S_G, I_G)$ is defined as follows:

\begin{itemize}
  \item The set $S_G$ is the set of edges in the graph $G$
  \item If $A$ is a subset of $E$ (edges), then $A \in I_G$ if and only if $A$ is acyclic. That is, a set of edges $A$ is independent if and only if the subgraph $G_A = (V,A)$ forms a forest
\end{itemize}

\begin{theorem}
  If $G = (V,E)$ is an undirected graph, then $M_G = (S_G,I_G)$ is a matroid.
\end{theorem}

\begin{theorem}
  All maximal independent subsets in a matroid have the same size.
\end{theorem}

Given a matroid $M$, we call an element $x \notin A$ an \textbf{extension} of $A \in I$ if we can add $x$ to $A$ while preserving its independence, i.e. $A \cup \{x\} \in I$. 

A matroid $M = (S,I)$ is said to be weighted if it is associated with weight function $w(x)$ for all $x \in S$. $w(A)$ is defined as :

\[w(A) = \sum_{x \in A} w(x)\]

The independent set with maximum $w(A)$ is called an optimal subset of a matroid. An optimal subset is always a maximal independent subset.

Let us consider the Minimum Spanning Tree problem, where we seek the subset of edges that connects all the vertices together and has minimum total length. This is like finding the optimal subset of a weighted matroid $M_G$ where weight function $w'(e) = w_0 - w(e)$, where $w(e)$ is the weight of the edge and $w_0$ is some constant greater than all the weights. A greedy algorithm for a weighted matroid is:


\begin{algorithm}[H]
  \SetAlgoLined
  $A = \phi$ \\
  sort $M.S$ into monotonically decreasing order of weight $w$  \\
  \For{each $x \in M.S$}{
    \If{$A \cup \{x\} \in M.I$}
    {
      $A = A \cup \{x\}$\\
    }
  }
  \Return A \\
  \caption{Greedy(M,w)}
\end{algorithm}

\begin{lemma}[Greedy Choice Property]
  Consider $M = (S,I)$ with weight function $w$. Let $S$ be sorted in decreasing order. Consider $x$, the first element of $S$ such that $\{x\} $ is independent. If this exists then there exists an optimal subset $A$ containing $x$.
\end{lemma}

\begin{lemma}
  Let $M = (S,I)$ be any matroid. If $x$ is an element of $S$ that is an extension of some independent subset $A$ of $S$, then $x$ is also an extension of $\phi$. 
\end{lemma}

\begin{corollary}
  Let $M = (S,I)$ be any matroid. If $x$ is an element of $S$ such that $x$ is not an extension of $\phi$, then $x$ is not an extension of any independent set $A$ of $S$. 
\end{corollary}

\begin{lemma}[Optimal substructure property]
  Let $x$ be the first element of $S$ chosen by GREEDY for the weighted matroid $M = (S,I)$. We can reduce this problem to $M' = (S',I')$, such that:
  \begin{itemize}
    \item $s' = \{y \in S : \{x,y\} \in I\}$
    \item $I' = \{B \subseteq S - \{x\} : B \cup \{x\} \in I\}$
  \end{itemize}
\end{lemma}


\section{0-1 Knapsack}

No notes for this, it's too simple. $O(N*W)$ algorithm. For general information, there are lots of faster algorithms if you add some extra constraints.

\section{Travelling Salesman Problem}

Consider we have a graph, where every edge between vertices $i$ and $j$ has some weight $c_{ij}$. Our goal is to find a path where we start from one city, visit every other city and return to the same one again, in the cheapest manner. This is like finding a Hamiltonian Cycle in the graph.

Let $g(i,S)$ be the length of the shortest path starting at vertex $i$, going through all the vertices in $S$ and terminating at 1. Then the following equations are obvious:

\[g(1,V-\{1\}) = \min_{2 \leq k \leq n} \{c_{1k} + g(k,V-\{1,k\})\}\]
\[g(i,S) = \min_{j \in S} \{c_{ij} + g(j,S-\{j\})\}\]

From this, we can design the TSP algorithm:

\begin{algorithm}[H]
  \SetAlgoLined
  \For{$i = 2$ to $n$} {
    $g(i,\phi) = c_{j1}$
  }
  \For{$k = 1$ to $n-2$}{
    \For{$i=2$ to $n$} {
      \For{$S \subseteq V - \{i,1\}$ with $|S| = k$}{
        $g(i,S) = \min_{j \in S} \{c_{ij} + g(j, S-\{j\})\}$
      }
    }
  }
  $g(1,V-\{1\}) = \min_{j \in S} \{c_{1i} + g(i,V-\{1,i\})\}$ \\
  \Return $g(1,V-\{1\})$
  \caption{TSP($V,c_{ij}$)}
\end{algorithm}

The time complexity of this is $T(n) = \Theta(n^2 \cdot 2^n)$ and space complexity $\Theta(n2^n)$.

\section{Matrix Chain Multiplication}

If we are given a sequence of matrices, $A,B,C$ of size $u \times v$, $v \times w$, $w \times z$ respectively. This gives us two ways to multiple the matrix : 

\begin{itemize}
  \item $(A \times B) \times C$ : Takes $u \times v \times w + u \times w \times z$ steps
  \item $A \times (B \times C)$ : Takes $u \times v \times z + v \times w \times z$ steps
\end{itemize}

Our goal is to find the order of multiplication that would take the minimum number of steps.

One way to do this could be brute force, where we try every order of multiplication. This problem is equivalent to finding the number of ways to parenthesize an expression of $n$ matrices. This can be expression by the recursion:

\[P(n) = \begin{cases}
1 & \text{if } n = 1 \\
\sum_{k=1}^{n-1} P(k) \times P(n-k) & \text{otherwise}
\end{cases}
\]
This is, in fact, the $n-1$ Catalan number $C(n-1)$, where:
\[C(n) = \frac{1}{n+1} {2n \choose n}\]

A more efficient approach to solve this is DP. Let us assume every matrix $A_i$ has the dimensions $p_{i-1} \times p_i$. Then we can use the following DP:

\begin{algorithm}[H]
  \SetAlgoLined
  $n$ = length[$p$] - 1 \\
  \For{i = 1 to n} {
    $m[i][i] = 0$
  }
  \For{l = 2 to n} {
    \For{i=1 to n-l+1} {
      $j=i+l-1$ \\
      $m[i][j] = \infty$ \\
      \For{k=i to j-1} {
        $q = m[i][k] + m[k+1[j] + p_{i-1}p_kp_j$ \\
        \If{$q < m[i][j]$}{
          $m[i][j] = q$ \\
          $s[i][j] = k$ 
        }
      }
    }
  }
  \caption{Matrix-Chain-Order(p)}
\end{algorithm}

This is a standard DP by length. The idea is, if we are given a range of matrices $[l .. r]$, the best parenthesization will always involve splitting the range into two parts - a prefix and a suffix, and multiplying it. So, for every range, we store the best achievable cost for every subrange of a length $l$, and to find the cost of a current subrange, try every prefix to find the minimum cost.

\section{Longest Common Subsequence}

This is also very common and standard, read GFG or something. Interesting fact is that the longest palindromic subsequence in a string is the LCS of the string and it's reverse.

\section{Optimal Binary Search Trees}

Suppose that we are designing a program to translate text from English to French. For each occurrence of English word in the text, we need to look up it's French equivalent. This can be done using a binary tree, and could ensure $O(\log n)$ time. However, words occur at different frequencies, so there could be a different total cost of search given a text. So, we want a optimal binary search tree.

Formally, we are given $n$ keys $K = k_1,k_2,...,k_n$ in sorted order and wish to build a BST on these keys. For each $k_i$, we have a $p_i$ probability that the search will be for $k_i$. Some searches may be for values not in $K$, so we also have $n+1$ dummy keys $d_0,d_1,...d_n$. In particular, $d_i$ represents values between $k_i$ and $k_{i+1}$. Each of these have a probability $q_i$. Of course,

\[\sum_{i=1}^n p_i + \sum_{i=0}^n q_i = 1\]

The expected cost in a tree T is given by:

\[E[T] = \sum_{i=1}^n (depth_{T}(k_i) + 1) \times p_i + \sum_{i=0}^n (depth_{T}(d_i) + 1)\times q_i \]

Any non-leaf subtree of our BST must contain keys in a continuous range $k_i \cdots k_j$. Each subtree must be optimal. Let $e[i,j]$ be the expected cost for an optimal BST of keys $k_i,\cdots,k_j$, and let $w[i,j]$ be such that:

\[w[i,j] = \sum_{v=i}^j p_v + \sum_{v=i-1}^j q_v\]

Then if $k_r$ is root,
\begin{align*}
  e[i,j] &=  p_r + e[i,r-1] + w[i,r-1] + e[r+1,j] + w[r+1,j] \\
         &= e[i,r-1] + e[r+1,j] + w[i,j]
\end{align*}

Hence our goal becomes choosing $r$ such that it minimizes $e[i,j]$. This gives us the following algo

\begin{algorithm}[H]
  \SetAlgoLined
  \For{i = 1 to n+1} {
    $e[i,i-1] = w[i,i-1] = q_i-1$ \\
  }
  \For{l = 1 to n} {
    \For{i = 1 to n-l+1} {
      $j = i-l+1$ \\
      $e[i,j] = \infty$ \\ 
      $w[i,j] = w[i,j-1] + p_j + q_j$ \\
      \For{r = i to j} {
        $t = e[i,r-1] + e[r+1,j] + w[i,j]$ \\
        \If{$t < e[i,j]$} {
          $e[i,j] = t$ \\
          $root[i,j] = r$
        }
      }
    }
  }
  \caption{Optimal-BST(p,q,n)}
\end{algorithm}

\end{document}
