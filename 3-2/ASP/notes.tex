\documentclass[12pt,letterpaper]{amsbook}

% Formatting packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1 in]{geometry}
\usepackage{parskip}
\usepackage[hidelinks]{hyperref}

% Picture packages
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

% AMS packages
\usepackage{amsmath,amsfonts,mathtools,amsthm,amssymb}

% Formatting
\renewcommand{\baselinestretch}{1.25}

% Theorems and other necessary structures
\usepackage{mdframed}
\mdfsetup{skipabove=1em,skipbelow=0em}
\theoremstyle{definition}
\newmdtheoremenv[nobreak=true]{theorem}{Theorem}[chapter] % Big result
\newmdtheoremenv[nobreak=true]{corollary}{Corollary}[theorem] % Follows from a theorem
\newmdtheoremenv[nobreak=true]{lemma}[theorem]{Lemma} % Minor result
\newtheorem{definition}{Definition}% Definition
\newtheorem*{remark}{Remark}
\newtheorem*{exercise}{Exercise}

\newenvironment{solution}
  {\renewcommand\qedsymbol{$\blacksquare$}\begin{proof}[Solution]}
  {\end{proof}}

% New commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

% Title information
\title{Applied Stochastic Processes}
\author{2018A7PS0193P}

\begin{document}

\maketitle

\chapter{Fundamentals}

\section{Stochastic Processes}

\begin{definition}
  A stochastic process is a probability model that describes the evolution of a system evolving randomly in time.
\end{definition}

\begin{definition}
  A random variable is a mapping $X : \Omega \rightarrow \R$ that assigns a real number $X(\omega)$ to each outcome $\omega \in \Omega$, where $\Omega$ is the sample space.
\end{definition}

A stochastic process can be given by a collection of random variables $\{X(t), t \in T\}$, where $T$ is called the \textbf{index set}. If $T$ is countable (observed at discrete times), we get a \textbf{discrete time stochastic process}. On the other hand, if $T$ is uncountable (observed continuously), then we get a \textbf{continuous time stochastic process}.

\begin{definition}
  The state space  of a stochastic process is defined as the set of all possible values that the random variables $X(t)$ can assume.
\end{definition}

\section{Elementary Probability}

For a recap of elementary probability, refer to the notes from Applied Statistical Methods.

\section{Transformation of Random Variables}

\phantom{Invisible text to fix mdframe, I don't want to switch to tcolorbox}

\begin{lemma}
  Let $X$ have a continuous, strictly increasing CDF $F$. Let $U \sim $ Uniform(0,1). If $Y = F^{-1}(U)$, then Y also has the CDF $F$. 
\end{lemma}

The lemma above allows us to transform $U$ to any other random variable, as long as it has a continuous and strictly increasing CDF. Say we had an algorithm to define a uniform random variable in the range (0,1), now we have a way to generate random variables from a different distribution.

\section{Moment Generating Functions}

\begin{definition}
  The moment generating function $\phi(t)$ of the random variable $X$ is defined for all values $t$ as $\phi(t) = E[e^{tx}]$.
\end{definition}

The moment generating functions of some oft-used distributions are as follows:

\begin{itemize}
  \item Moment generating function of Binomial($n,p$) is:
    \begin{align*}
      \phi(t) = (pe^t + 1 - p)^n
    \end{align*}
  \item Moment generating function of Poisson($\lambda$) is:
    \begin{align*}
      \phi(t) = e^{\lambda(e^t-1)}
    \end{align*}
  \item Moment generating function of Exponential($\lambda$) is:
    \begin{align*}
      \phi(t) = \frac{\lambda}{\lambda-t}
    \end{align*}
  \item TODO more
\end{itemize}

\begin{theorem}
  The moment generating function of the sum of independent random variables is the product of the individual moment generating functions.
\end{theorem}

So, this means that $\phi_{X+Y}(t) = \phi_X(t) \cdot \phi_Y(t)$, as long as $X \perp Y$ (this notation means that they are independent).

\section{Conditional distributions}

The conditional probability distribution of $Y$ given the occurrence of the value $x$ of $X$ is given by:

\[ f_{Y|X} (y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}\]

where $f_{X,Y}(x,y)$ is the joint distribution and $f_X(x)$ is the marginal density of $X$.

The conditional expectation of $X$ given $Y$ is:

\[E(X|Y=y) = \int_{-\infty}^{\infty} x f_{X|Y}(x,y) dx \]

\section{Markov's and Chebyshev's Inequality}

\begin{theorem}[Markov's Inequality]
  Let $X$ be a non-negative random variable and suppose that $E(X)$ exists. For any $t > 0$, 
  \[P(X > t) \leq \frac{E(X)}{t}\]
\end{theorem}

\begin{theorem}[Chebyshev's Inequality]
  Let $\mu = E(X)$ and $\sigma^2 = V(X)$. Then
  \[P(|X-\mu| \geq t) \leq \frac{\sigma^2}{t^2}\]
  \[ P(|Z| \geq k) \leq \frac{1}{k^2}\]
  where $Z = (X-\mu) / \sigma$ and $t > 0$.
\end{theorem}

Chebyshev's Inequality is a more general version of Markov's Inequality, applicable for any random variable $X$.

\section{Convergence of Random Variables}

Let $X_1,X_2,...$ be a sequence of random variables and let $X$ be another random variable. Let $F_n$ be the CDF of $X_n$ and $F$ be the CDF of $X$.

We say that $X_n$ converges to $X$ in probability, denoted by $X_n \xrightarrow{P} X$, if for every $\epsilon > 0$,
\[P(|X_n-X| > \epsilon) \rightarrow 0\]
as $n \rightarrow \infty$.

We say that $X_n$ converges to $X$ in distribution, denoted by $X_n \leadsto X$, if
\[ \lim_{n \rightarrow \infty} F_n(t) = F(t)\]
for all $t$ for which $F$ is continuous.

\begin{lemma}
  If $X_n \xrightarrow{P} X$, then $X_n \leadsto X$.
\end{lemma}

The above lemma is provided without proof, as it is beyond the scope of the course.

\begin{exercise}

Let $X_n \sim N(0,\frac{1}{n})$. Prove that this series converges to 

\[ F_X(n) = \begin{cases}
0 & x < 0 \\
1 & x \geq 0
\end{cases}\]

in distribution. 

\begin{solution}
Let $t > 0$, and define the standard normal variable $Z_n = \sqrt{n}X_n$, so $Z_n \sim N(0,1)$. So,
\begin{align*}
  F_{X_n}(t) &= P(X_n \leq t) \\
             &= P(Z_n \leq \sqrt{n}t) \\
             &= \int_{-\infty} ^{\sqrt{n}t} f(x) dx
\end{align*}

where $f(x)$ is the PDF of $Z_n$.

It is clear that as $n \rightarrow \infty$, we get the following distribution:

\[F_{X_n}(t) = \begin{cases}
  0 & t < 0 \\
  0.5 & t = 0 \\
  1 & t > 0
\end{cases}\]

So, $F_{X_n} \leadsto F_X \forall t \in \R - \{0\}$.
\end{solution}
\end{exercise}

\chapter{Markov Chains}

\section{Introduction}

\begin{definition}
  A stochastic process with a finite number of state spaces $S = \{0,1,...,N\}$ and a countable index state $T = \{t_0,t_1,t_2,...\}$ is a Markov chain if
  \[P(X_{n+1} = j|X_n = i, X_{n-1} = i_{n-1},...,X_0 = i_0) \\ \]
  \[= P(X_{n+1} = j | X_n = i) \]
  \[= \mathbf{P}_{ij}\]
\end{definition}

This means that the probability of a transition from state $i$ to a state $j$ is completely dependent on $i$ and $j$, and not on any history. The resulting transition matrix is called the \textbf{one step transition matrix}, denoted by $\textbf{P}$.

\begin{lemma}
  \[\sum_{j = 0}^{\infty} \mathbf{P}_{ij} = 1 \forall i \in S\]
\end{lemma}

Of course, this matrix means that we could represent a Markov Chain by a digraph, or even Petri Nets (out of syllabus). 

\section{State Probabilities}

Let $\mathbf{\Pi}^{(n)}$ be a row vector such that $\mathbf{\Pi}^{n}_i$ is the probability that after $n$ transitions, we are at state $i$. This is known as the \textbf{marginal probability distribution}. This vector can be recursively computed by the formula:
\[ \mathbf{\Pi}^{(n)} = \mathbf{\Pi}^{(n-1)} \mathbf{P} \]
So, we get
\[ \mathbf{\Pi}^{(n)} = \mathbf{\Pi}^{(0)} \mathbf{P}^n \]

$\mathbf{P}^n$ is known as the $n^{th}$ step transition probability matrix (denoted by $p^{(n)}$), where:
\[\mathbf{P}^{n}_{ij} = p_{ij}^{n} = P(X_n = j|X_0 = i)\]
This means that $p^{(n)}$ solves the question - after $n$ transitions starting from $i$, what is the probability that I will be in state $j$?

A problem we are facing with this approach is that we have to calculate $\mathbf{P}^{n}$ fast enough. It is not enough to use binary exponentiation and find it in $O(d^3\log{n})$, since we are using matrices with large size, so while it would be fast in exponent, it would be slow doing matrix multiplication. Instead, we use \textbf{Eigendecomposition} (read FDS notes).

Using eigendecomposition, we can decompose $\mathbf{P}$ into:
\[\mathbf{P} = Q \Lambda Q^{-1}\]
Raising it to power of $n$, we get the decomposition to be:
\[\mathbf{P}^n = Q \Lambda^n Q^{-1}\]
Since $\Lambda$ is a diagonal matrix, we can find the exponent even faster in $O(d\log n)$, hence speeding up the process.

\begin{theorem}[Chapman-Kolmogorov Equation]
  \[p_{jk}^{(m+n)} = \sum p^{(n)}_{rk} p^{(m)}_{jr} = \sum p_{jr}^{(n)} p_{rk}^{(m)}\]
\end{theorem}

\begin{theorem}
  If $\mathbf{P}$ is a transition matrix for a finite state Markov chain, it has at least one eigenvalue as 1.  All the other eigenvalues have an absolute value $|\lambda_i| \leq$ 1.
\end{theorem}

\begin{definition}
  The stationary distribution of a Markov Chain is a row vector $\mathbf{\Pi}$ such that 
  \[\mathbf{\Pi} \cdot \mathbf{P} = \mathbf{\Pi}\]
\end{definition}

So, the stationary distribution $\mathbf{\Pi}$ is the left eigenvector of $\mathbf{P}$ such that it's eigenvalue is 1.

\begin{definition}
  If $\lim_{n \rightarrow \infty} \mathbf{P}^n$ is such that all the rows in it are equal, that row is said to be the limiting distribution.
\end{definition}

The limiting distribution may not always exist, but if it does, it is equivalent to the stationary distribution.

\section{Occupancy Time and First Entrance Time}

\begin{definition}
  Let $N_{ij}^{(n)}$ be the number of times a discrete time Markov Chain visits a state $j$ starting from state $i$ over a given time span of $n$. The occupancy time for state $j$ starting from $i$ is:
  \[T_{ij}^{(n)} = E(N_{ij}^{(n)})\]
\end{definition}

\begin{theorem}
  The occupancy times matrix $\mathbf{T}^{(n)} = \sum_{k=0}^{n} \mathbf{P}^{k}$.  
\end{theorem}

\begin{definition}
  Let $f_{ij}^{(n)}$ be the probability the Markov Chain visits a state $j$ for the first time starting from a state $i$ after $n$ transitions. Then $f$ is the First Entrance Time Matrix.
\end{definition}

\begin{theorem}
  \[f_{ij}^{(n)} = P_{ij}^{(n)} - \sum_{r=0}^{n-1} f_{ij}^{(r)} P_{jj}^{n-r}\]
\end{theorem}

\begin{definition}
The mean time of eventual return is the expected number of transitions to go from $i$ to $j$. This is denoted by $\mu_{ij}$ and is given by:
\[\mu_{ij} = \sum_{n=1}^{\infty} n f_{ij}^{(n)}\]
\end{definition}

$\mu_{ii}$ is called the mean recurrence time. This value is not guaranteed to converge.

\section{Classification of States}

\begin{definition}[accessibility]
  We say that $i$ reaches $j$ (or $j$ is accessible from $i$) if $P^{n}_{ij} > 0$ for some $n$, and we denote it by $i \rightarrow j$.
\end{definition}

\begin{definition}  [communicability]
  If $i \rightarrow j$ and $j \rightarrow i$, then we write $i \leftrightarrow j$ and we say that $i$ and $j$ communicate.
\end{definition}

\begin{theorem}
  The communication relation satisfies the following properties:
  \begin{enumerate}
    \item $i \leftrightarrow i$ (reflexive)
    \item $i \leftrightarrow j \Rightarrow j \leftrightarrow i$ (symmetric)
    \item If $i \leftrightarrow j$ and $\j \leftrightarrow k$ then $i \leftrightarrow k$ (transitive)
    \item The set of states $\chi$ can be written as a disjoint union of classes $\chi = \chi_1 \cup \chi_2 \cup ...$ where two states $i$ and $j$ communicate with each other if and only if they are in the same class. (equivalence class)
  \end{enumerate}
  It is hence an equivalence relation.
\end{theorem}

\begin{remark}
  This has it's own mathematical proof from the definition, but it is far more intuitive to think of the strongly connected components in a directed graph.  
\end{remark}

If all states communicate with each other, then the chain is said to be \textbf{irreducible}. A set of states is \textbf{closed} if, once you enter that set of states you never leave. In a more mathematical sense, a set $B$ is closed if for all $j \in B^C$, there is no $i \in B$ such that $i \rightarrow j$. A closed set consisting of a single state is called an \textbf{absorbing state}.   

\begin{definition}
  State $i$ is recurrent or persistent if 
  \[P(X_n=i \text{ for some } n \geq 1 | X_0 = i)  = 1\]
  Otherwise, the state is transient.
\end{definition}

A recurrent state is \textbf{null} if $\mu_{ii} = \infty$. Otherwise it is called \textbf{positive} or non null.

\begin{theorem}
  A state $i$ is recurrent if and only if $\sum_n P_{ii}^n = \infty$. A state $i$ is transient if and only if $\sum_n P_{ii}^n < \infty$  
\end{theorem}

This means that if $i$ is a persistent state, then if every state in it's equivalence class is also persistent, i.e. it is a class property.

\begin{theorem}
  For a finite Markov Chain, there exists at least one persistent class.
\end{theorem}

\begin{theorem}
  A set B is closed if and only if $P_{ij} = 0 \forall j \in B^C, i \in B$.  
\end{theorem}
\begin{remark}
  This comes directly from thinking of the Markov Chain as a graph.
\end{remark}

\begin{theorem}
  If $B$ is a closed set, then $B$ is the union of equivalence classes in the Markov Chain.
\end{theorem}

\begin{definition}
  The period of a state $i$ is $d_i = gcd(\{n : P_{ii}^n > 0\})$. If $d_i > 1$, the state is periodic, and is otherwise aperiodic if $d_i=1$.
\end{definition}

Periodicity is a property of a class. Every state in the same class has the same period.

\begin{definition}
  A state is ergodic if it is persistent and aperiodic. A chain is ergodic if all it's states are ergodic. 
\end{definition}

Ergodism is a class property, since periodicity and persistency are both class properties.

\chapter{Poisson Processes}

\begin{definition}
Let $X$ be a continuous random variable whose values lie in the non negative real numbers. The probability distribution of $X$ is \textbf{memoryless} if for any non negative real numbers $t$ and $s$, we have:
\[P(X>t + s | X > t) = P(X>s)\]
\end{definition}

Hence, the distribution of a ``waiting time'' until a certain event does not depend on how much time has elapsed already.

\begin{theorem}
  A positive continuous random variable $X$ is memoryless if and only if $X \sim Exp(x;\lambda)$ for some $\lambda$.
\end{theorem}

\begin{definition}
A stochastic process $\{N(t),t \geq 0\}$ is said to be a counting process if $N(t)$ represents the total number of events that occur by time $t$.  
\end{definition}

From it's definition, 
\begin{itemize}
  \item $N(t) \geq 0$
  \item $N(t)$ is integer valued
  \item If $s < t$ then $N(s) \leq N(t)$
  \item For $s < t$, $N(t)-N(s)$ equals the number of events that occur in the interval $(s,t]$
\end{itemize}

A counting process has \textbf{independent increments}  if the numbers of events that occur in disjoint time intervals are independent. A counting process has \textbf{stationary increments} if the distribution of the number of events that occur in any time interval only depends on the length of that time interval.

\begin{definition}
  The counting process $\{N(t),t\}$ is said to be a Poisson Process having rate $\lambda, \lambda > 0$ if 
  \begin{enumerate}
    \item $N(0) = 0$
    \item The process has independent increments
    \item The number of events in any interval of length $t$ is Poisson distributed with mean $\lambda t$. That is, for all $s,t \geq 0$
      \[P(N(t+s) - N(s) = n) = e^{- \lambda t}\frac{(\lambda t) ^ n}{n!}\]
      This means it has stationary increments.
  \end{enumerate}
\end{definition}

The definition of a Poisson process can also be encoded in the form of three postulates, which must be present for a counting process to be a Poisson process:

\begin{enumerate}
  \item \textbf{Independence} : $N(t+h) - N(t)$ is independent of the number of occurrences prior to that interval
  \item \textbf{Homogeneity in time} : $p_n(t)$ depends only on the length $t$ of the interval and is independent of where the interval is situated, where:
    \[p_n(t) = P(N(t+s) - N(s) = n)\]
  \item \textbf{Regularity} : In an interval of infinitesimal length $h$, the probability of exactly one occurrence $\lambda h + o(h)$ and that of more than one occurrence is $o(h)$ (wut?)
\end{enumerate}

It is possible to prove that these two definitions are equivalent.

\begin{lemma}
  For a Poisson Process with rate $\lambda$,
  \[E[N(t)] = \lambda t\]
\end{lemma}


\end{document}
