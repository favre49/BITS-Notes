\documentclass[12pt,letterpaper]{amsbook}

% Formatting packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1 in]{geometry}
\usepackage{parskip}
\usepackage[hidelinks]{hyperref}

% Picture packages
\usepackage{graphicx}
\usepackage[justification=centering]{caption}

% AMS packages
\usepackage{amsmath,amsfonts,mathtools,amsthm,amssymb}

% Formatting
\renewcommand{\baselinestretch}{1.25}

% Theorems and other necessary structures
\usepackage{mdframed}
\mdfsetup{skipabove=1em,skipbelow=0em}
\theoremstyle{definition}
\newmdtheoremenv[nobreak=true]{theorem}{Theorem}[chapter] % Big result
\newmdtheoremenv[nobreak=true]{corollary}{Corollary}[theorem] % Follows from a theorem
\newmdtheoremenv[nobreak=true]{lemma}[theorem]{Lemma} % Minor result
\newtheorem{definition}{Definition}% Definition
\newtheorem*{remark}{Remark}
\newtheorem*{exercise}{Exercise}

\newenvironment{solution}
  {\renewcommand\qedsymbol{$\blacksquare$}\begin{proof}[Solution]}
  {\end{proof}}

% New commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}

% Title information
\title{Applied Stochastic Processes}
\author{2018A7PS0193P}

\begin{document}

\maketitle

\chapter{Fundamentals}

\section{Stochastic Processes}

\begin{definition}
  A stochastic process is a probability model that describes the evolution of a system evolving randomly in time.
\end{definition}

\begin{definition}
  A random variable is a mapping $X : \Omega \rightarrow \R$ that assigns a real number $X(\omega)$ to each outcome $\omega \in \Omega$, where $\Omega$ is the sample space.
\end{definition}

A stochastic process can be given by a collection of random variables $\{X(t), t \in T\}$, where $T$ is called the \textbf{index set}. If $T$ is countable (observed at discrete times), we get a \textbf{discrete time stochastic process}. On the other hand, if $T$ is uncountable (observed continuously), then we get a \textbf{continuous time stochastic process}.

\begin{definition}
  The state space  of a stochastic process is defined as the set of all possible values that the random variables $X(t)$ can assume.
\end{definition}

\section{Elementary Probability}

For a recap of elementary probability, refer to the notes from Applied Statistical Methods.

\section{Transformation of Random Variables}

\phantom{Invisible text to fix mdframe, I don't want to switch to tcolorbox}

\begin{lemma}
  Let $X$ have a continuous, strictly increasing CDF $F$. Let $U \sim $ Uniform(0,1). If $Y = F^{-1}(U)$, then Y also has the CDF $F$. 
\end{lemma}

The lemma above allows us to transform $U$ to any other random variable, as long as it has a continuous and strictly increasing CDF. Say we had an algorithm to define a uniform random variable in the range (0,1), now we have a way to generate random variables from a different distribution.

\section{Moment Generating Functions}

\begin{definition}
  The moment generating function $\phi(t)$ of the random variable $X$ is defined for all values $t$ as $\phi(t) = E[e^{tx}]$.
\end{definition}

The moment generating functions of some oft-used distributions are as follows:

\begin{itemize}
  \item Moment generating function of Binomial($n,p$) is:
    \[ \phi(t) = (pe^t + 1 - p)^n\]
  \item Moment generating function of Poisson($\lambda$) is:
    \[  \phi(t) = e^{\lambda(e^t-1)}\]
  \item Moment generating function of Exponential($\lambda$) is:
    \begin{align*}
      \phi(t) = \frac{\lambda}{\lambda-t}
    \end{align*}
  \item Moment generating function for Normal($\mu, \sigma^2$) is:
    \[e^{t \mu + \frac{1}{2} \sigma^2 t^2}\]
  \item Moment generating function for Uniform($a,b$) is :
    \[\frac{e^{tb} - e^{ta}}{t(b-a)}\]
\end{itemize}

\begin{theorem}
  The moment generating function of the sum of independent random variables is the product of the individual moment generating functions.
\end{theorem}

So, this means that $\phi_{X+Y}(t) = \phi_X(t) \cdot \phi_Y(t)$, as long as $X \perp Y$ (this notation means that they are independent).

\section{Conditional distributions}

The conditional probability distribution of $Y$ given the occurrence of the value $x$ of $X$ is given by:

\[ f_{Y|X} (y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}\]

where $f_{X,Y}(x,y)$ is the joint distribution and $f_X(x)$ is the marginal density of $X$.

The conditional expectation of $X$ given $Y$ is:

\[E(X|Y=y) = \int_{-\infty}^{\infty} x f_{X|Y}(x,y) dx \]

\section{Markov's and Chebyshev's Inequality}

\begin{theorem}[Markov's Inequality]
  Let $X$ be a non-negative random variable and suppose that $E(X)$ exists. For any $t > 0$, 
  \[P(X > t) \leq \frac{E(X)}{t}\]
\end{theorem}
\begin{proof}
  \[E(X) = P(X < a) \cdot E(X|X < a) + P(X \geq a) \cdot E(X|X \geq a)\] 
  Here, $E(X|X<a)$ is larger than 0 since $X$ is a non negative random variable. $E(X|X \geq a)$ is larger than $a$, since it only considers values larger than $a$. Hence,
  \[E(X) \geq P(X<a)\cdot 0 + P(X \geq a) \cdot E(X|X \geq a) \geq a \cdot P(X \geq a) \]
  This directly leads to Markov's Inequality.
\end{proof}

\begin{theorem}[Chebyshev's Inequality]
  Let $\mu = E(X)$ and $\sigma^2 = V(X)$. Then
  \[P(|X-\mu| \geq t) \leq \frac{\sigma^2}{t^2}\]
  \[ P(|Z| \geq k) \leq \frac{1}{k^2}\]
  where $Z = (X-\mu) / \sigma$ and $t > 0$.
\end{theorem}
\begin{proof}
  \begin{align*}
    \sigma^2 &= E((X-\mu)^2)  \\
             &= E((X-\mu)^2 | k \sigma \leq |X-\mu|) P(k \sigma \leq |X - \mu|) + E((X_\mu)^2 | k \sigma > |X-\mu|)P(k\sigma > |X - \mu|) \\
             &\geq (k \sigma)^2 P(k \sigma \leq |X-\mu|) + 0 \cdot P(k \sigma > |X-\mu|) \\
             &= k^2 \sigma^2 P(k \sigma \leq |X-\mu|)
  \end{align*} 
  Now the inequality follows from dividing by $k^2 \sigma^2$.
\end{proof}

Chebyshev's Inequality is a more general version of Markov's Inequality, applicable for any random variable $X$.

\section{Convergence of Random Variables}

Let $X_1,X_2,...$ be a sequence of random variables and let $X$ be another random variable. Let $F_n$ be the CDF of $X_n$ and $F$ be the CDF of $X$.

We say that $X_n$ converges to $X$ in probability, denoted by $X_n \xrightarrow{P} X$, if for every $\epsilon > 0$,
\[P(|X_n-X| > \epsilon) \rightarrow 0\]
as $n \rightarrow \infty$.

We say that $X_n$ converges to $X$ in distribution, denoted by $X_n \leadsto X$, if
\[ \lim_{n \rightarrow \infty} F_n(t) = F(t)\]
for all $t$ for which $F$ is continuous.

\begin{lemma}
  If $X_n \xrightarrow{P} X$, then $X_n \leadsto X$.
\end{lemma}

The above lemma is provided without proof, as it is beyond the scope of the course.

\begin{exercise}

Let $X_n \sim N(0,\frac{1}{n})$. Prove that this series converges to 

\[ F_X(n) = \begin{cases}
0 & x < 0 \\
1 & x \geq 0
\end{cases}\]

in distribution. 

\begin{solution}
Let $t > 0$, and define the standard normal variable $Z_n = \sqrt{n}X_n$, so $Z_n \sim N(0,1)$. So,
\begin{align*}
  F_{X_n}(t) &= P(X_n \leq t) \\
             &= P(Z_n \leq \sqrt{n}t) \\
             &= \int_{-\infty} ^{\sqrt{n}t} f(x) dx
\end{align*}

where $f(x)$ is the PDF of $Z_n$.

It is clear that as $n \rightarrow \infty$, we get the following distribution:

\[F_{X_n}(t) = \begin{cases}
  0 & t < 0 \\
  0.5 & t = 0 \\
  1 & t > 0
\end{cases}\]

So, $F_{X_n} \leadsto F_X \forall t \in \R - \{0\}$.
\end{solution}
\end{exercise}

\chapter{Markov Chains}

\section{Introduction}

\begin{definition}
  A stochastic process with a finite number of state spaces $S = \{0,1,...,N\}$ and a countable index state $T = \{t_0,t_1,t_2,...\}$ is a Markov chain if
  \[P(X_{n+1} = j|X_n = i, X_{n-1} = i_{n-1},...,X_0 = i_0) \\ \]
  \[= P(X_{n+1} = j | X_n = i) \]
  \[= \mathbf{P}_{ij}\]
\end{definition}

This means that the probability of a transition from state $i$ to a state $j$ is completely dependent on $i$ and $j$, and not on any history. The resulting transition matrix is called the \textbf{one step transition matrix}, denoted by $\textbf{P}$.

\begin{lemma}
  \[\sum_{j = 0}^{\infty} \mathbf{P}_{ij} = 1 \forall i \in S\]
\end{lemma}

Of course, this matrix means that we could represent a Markov Chain by a digraph, or even Petri Nets (out of syllabus). 

\section{State Probabilities}

Let $\mathbf{\Pi}^{(n)}$ be a row vector such that $\mathbf{\Pi}^{(n)}_i$ is the probability that after $n$ transitions, we are at state $i$. This is known as the \textbf{marginal probability distribution}. This vector can be recursively computed by the formula:
\[ \mathbf{\Pi}^{(n)} = \mathbf{\Pi}^{(n-1)} \mathbf{P} \]
So, we get
\[ \mathbf{\Pi}^{(n)} = \mathbf{\Pi}^{(0)} \mathbf{P}^n \]

$\mathbf{P}^n$ is known as the $n^{th}$ step transition probability matrix (denoted by $p^{(n)}$), where:
\[\mathbf{P}^{n}_{ij} = p_{ij}^{(n)} = P(X_n = j|X_0 = i)\]
This means that $p^{(n)}$ solves the question - after $n$ transitions starting from $i$, what is the probability that I will be in state $j$?

A problem we are facing with this approach is that we have to calculate $\mathbf{P}^{n}$ fast enough. It is not enough to use binary exponentiation and find it in $O(d^3\log{n})$, since we are using matrices with large size, so while it would be fast in exponent, it would be slow doing matrix multiplication. Instead, we use \textbf{Eigendecomposition} (read FDS notes).

Using eigendecomposition, we can decompose $\mathbf{P}$ into:
\[\mathbf{P} = Q \Lambda Q^{-1}\]
Raising it to power of $n$, we get the decomposition to be:
\[\mathbf{P}^n = Q \Lambda^n Q^{-1}\]
Since $\Lambda$ is a diagonal matrix, we can find the exponent even faster in $O(d\log n)$, hence speeding up the process.

\begin{theorem}[Chapman-Kolmogorov Equation]
  \[p_{ij}^{(n+m)} = \sum_{k=0}^{\infty} p_{ik}^{(n)}p_{kj}^{(m)}\]
\end{theorem}
\begin{proof}
  \begin{align*}
    p_{ij}^{(n+m)} &= P(X_{n+m} = j | X_0 = i) \\
                   &= \sum_{k=0}^{\infty} P(X_{n+m} = j, X_n = k | X_0 = i) \\ 
                   &= \sum_{k=0}^{\infty} P(X_{n+m} = j | X_n=k, X_0 = i) P(X_n = k | X_0 =i) \\
                   &= \sum_{k=0}^{\infty} P_{kj}^m P_{ik}^n
  \end{align*}  
\end{proof}

\begin{theorem}
  If $\mathbf{P}$ is a transition matrix for a finite state Markov chain, it has at least one eigenvalue as 1.  All the other eigenvalues have an absolute value $|\lambda_i| \leq$ 1.
\end{theorem}

\begin{definition}
  The stationary distribution of a Markov Chain is a row vector $\mathbf{\Pi}$ such that 
  \[\mathbf{\Pi} \cdot \mathbf{P} = \mathbf{\Pi}\]
\end{definition}

So, the stationary distribution $\mathbf{\Pi}$ is the left eigenvector of $\mathbf{P}$ such that it's eigenvalue is 1.

\begin{definition}
  The probability distribution $\mathbf{\Pi}$ is called the limiting distribution of a Markov chain if:
  \[\mathbf{\Pi}_j = \lim_{n \rightarrow \infty} P(X_n = j | X_0 = i)\]
  for all $i,j \in S$ and 
  \[\sum_{j \in S} \mathbf{\Pi}_j = 1\]
\end{definition}

The limiting distribution may not always exist, but if it does, it is equivalent to the stationary distribution. In fact, the limiting distribution does not depend on the start state.

The limiting distribution only exists if $\lim_{n \rightarrow \infty} P^n$ has all equal rows, and that row will be equal to the limiting distribution.

\begin{exercise}
  Consider a Markov chain with two possible states $S = \{0,1\}$  and the transition matrix:
  \[
  \begin{bmatrix}
    1-a & a \\
    b & 1-b \\
  \end{bmatrix}
  \]
  where $0 < a < 1, 0 < b < 1$. Find the limiting distribution $\pi$ of this Markov Chain.
\end{exercise}
\begin{solution}
  Let $\pi = [\pi_0, \pi_1]$. Then, we can write:
  \[\pi = \pi P\]
  From this, we get the equation:
  \[\pi_0 a = \pi_1 b\]
  Since $\pi$ must be a valid probability distribution,
  \[\pi_0 + \pi_1 = 1\]
  Thus, we can obtain a unique solution:
  \[ \pi =  \begin{bmatrix}
  \frac{b}{a+b} & \frac{a}{a+b} \\
\end{bmatrix} \]
\end{solution}

\section{Occupancy Time and First Entrance Time}

\begin{definition}
  Let $N_{ij}^{(n)}$ be the number of times a discrete time Markov Chain visits a state $j$ starting from state $i$ over a given time span of $n$. The occupancy time for state $j$ starting from $i$ is:
  \[T_{ij}^{(n)} = E(N_{ij}^{(n)})\]
\end{definition}

\begin{theorem}
  The occupancy times matrix $\mathbf{T}^{(n)} = \sum_{k=0}^{n} \mathbf{P}^{k}$.  
\end{theorem}

\begin{definition}
  Let $f_{ij}^{(n)}$ be the probability the Markov Chain visits a state $j$ for the first time starting from a state $i$ after $n$ transitions. Then $f$ is the First Entrance Time Matrix.
\end{definition}

Of course, the probability that a system starting at $i$ will ever reach $j$ is:

\[F_{ij} = \sum_{n=1}^{\infty} f_{ij}^{(n)}\]

$F_{ij}$ is called the probability of eventual return.

\begin{theorem}[First Entrance Theorem]
  \[p_{jk}^{(n)} = \sum_{r=0}^n f_{jk}^{(r)}p_{kk}^{(n-r)}, n \geq 1\]
  where
  \[p_{kk}^{(0)} = 1, f_{jk}^{(0)} = 0, f_{jk}^{(1)} = p_{jk}\]
\end{theorem}

The proof of this is rigorous and uses the strong Markov Property, so it is not included here.


\begin{definition}
The mean time of eventual return is the expected number of transitions to go from $i$ to $j$. This is denoted by $\mu_{ij}$ and is given by:
\[\mu_{ij} = \sum_{n=1}^{\infty} n f_{ij}^{(n)}\]
\end{definition}

$\mu_{ii}$ is called the mean recurrence time. This value is not guaranteed to converge.

\section{Classification of States}

\begin{definition}[accessibility]
  We say that $i$ reaches $j$ (or $j$ is accessible from $i$) if $P^{n}_{ij} > 0$ for some $n$, and we denote it by $i \rightarrow j$.
\end{definition}

\begin{definition}  [communicability]
  If $i \rightarrow j$ and $j \rightarrow i$, then we write $i \leftrightarrow j$ and we say that $i$ and $j$ communicate.
\end{definition}

\begin{theorem}
  The communication relation satisfies the following properties:
  \begin{enumerate}
    \item $i \leftrightarrow i$ (reflexive)
    \item $i \leftrightarrow j \Rightarrow j \leftrightarrow i$ (symmetric)
    \item If $i \leftrightarrow j$ and $\j \leftrightarrow k$ then $i \leftrightarrow k$ (transitive)
    \item The set of states $\chi$ can be written as a disjoint union of classes $\chi = \chi_1 \cup \chi_2 \cup ...$ where two states $i$ and $j$ communicate with each other if and only if they are in the same class. (equivalence class)
  \end{enumerate}
  It is hence an equivalence relation.
\end{theorem}

\begin{remark}
  This has it's own mathematical proof from the definition, but it is far more intuitive to think of the strongly connected components in a directed graph.  
\end{remark}

If all states communicate with each other, then the chain is said to be \textbf{irreducible}. A set of states is \textbf{closed} if, once you enter that set of states you never leave. In a more mathematical sense, a set $B$ is closed if for all $j \in B^C$, there is no $i \in B$ such that $i \rightarrow j$. A closed set consisting of a single state is called an \textbf{absorbing state}.   

\begin{definition}
  State $i$ is recurrent or persistent if 
  \[P(X_n=i \text{ for some } n \geq 1 | X_0 = i)  = 1\]
  Otherwise, the state is transient.
\end{definition}

This definition means that if a state is recurrent, then if we start at a state $i$, we will definitely return to that state after some $n > 0$ steps, i.e, $F_{ii} = 1$. Since this is essentially a recursive process (start from state $i$, return to state $i$, start again), the process will re-enter $i$ again and again and again. 

However, if a state $i$ is transient, there is some probability $p < 1$ ($p$ is in fact $F_{ii}$) that it will re-enter the state. Hence, the probability that the process will be in state $i$ for exactly $n$ time periods is:
\[p^{n-1} (1-p)\]

A recurrent state is \textbf{null} if $\mu_{ii} = \infty$. Otherwise it is called \textbf{positive} or non null. In a finite state Markov chain, all recurrent states are positive recurrent.

\begin{theorem}
  A state $i$ is recurrent if and only if $\sum_n P_{ii}^n = \infty$. A state $i$ is transient if and only if $\sum_n P_{ii}^n < \infty$  
\end{theorem}
\begin{proof}
  From our discussion, it is clear that a state $i$ is recurrent if and only if the expected number of time periods that the process is in state $i$ is infinite. Let $I_n$ be such that:
  \[I_n = \begin{cases}
    1, & \text{if } X_n = i \\
    0, & \text{if } X_n \neq i
  \end{cases}\]
  Hence, $\sum_{n=0}^{\infty} I_n$ is the number of periods that a process in a state $i$.
  \begin{align*}
    E \left( \sum_{n=0}^{\infty} I_n | X_0 = i\right) &= \sum_{n=0}^{\infty} E(I_n | X_0 = i) \\
                                                      &= \sum_{n=0}^{\infty} P(X_n = i | X_0 = i) \\
                                                      &= \sum_{n=0}^{\infty} P_{ii}^n
  \end{align*}
  Hence, if the state is recurrent, this value must be infinite. Otherwise, it would be recurrent.
\end{proof}

Note that this theorem also implies that if $i$ is transient, $P_{ii}^n \rightarrow 0$ as $n \rightarrow \infty$.

\begin{exercise}
  Prove that in a finite state Markov Chain, all states cannot be transient 
\end{exercise}
\begin{solution}
Let us assume we have a Markov Chain where all states are transient. If this were the case, there would be some finite amount of time after which state $i$, will never be visited, for every state $i$. Hence, after some finite time, no states will be visited. But, the process must be in some state, so we have arrived at a contradiction. Hence all states cannot be transient.
\end{solution}

\begin{theorem}
  If state $i$ is recurrent and $i \leftrightarrow j$, then $j$ is recurrent.
\end{theorem}
\begin{proof}
  To prove this, notice that since $i \leftrightarrow j$, there exists integers $k$ and $m$ such that $P^k_{ij} > 0, P^{m}_{ji}$. For any integer $n$, 
  \[P_{jj}^{m+n+k} \geq P_{ji}^m P_{ii}^n P_{ij}^k\]
  This is because the LHS is the probability of going from state $j$ to state $j$ in $m+n+k$ steps, but the RHS requires this to occur along a particular path. Summing over all $n$, 
  \[\sum_{n=0}^{\infty} P_{jj}^{m+n+k} \geq P_{ji}^m P_{ij}^k \sum_{n=0}^{\infty} P_{ii}^n = \infty\]
  This is straightforward from theorem 2.7. From that same theorem, $j$ is recurrent.
\end{proof}

This means that if $i$ is a recurrent state, then if every state in it's equivalence class is also recurrent, i.e. it is a class property.

\begin{theorem}
  A set B is closed if and only if $P_{ij} = 0 \forall j \in B^C, i \in B$.  
\end{theorem}
\begin{remark}
  This comes directly from thinking of the Markov Chain as a graph.
\end{remark}

\begin{theorem}
  If $B$ is a closed set, then $B$ is the union of equivalence classes in the Markov Chain.
\end{theorem}
\begin{proof}
  Assume this is not true. Then, there is some states $i \in B$ and $j \in B^C$ such that $i \leftrightarrow j$. By definition of a closed state, there must be no $j \in B^C$ such that $i \rightarrow j$. This generates a contradiction. Hence, $j \in B$.
\end{proof}

\begin{definition}
  The period of a state $i$ is $d_i = gcd(\{n : P_{ii}^n > 0\})$. If $d_i > 1$, the state is periodic, and is otherwise aperiodic if $d_i=1$.
\end{definition}

Periodicity is a property of a class. Every state in the same class has the same period.

\begin{definition}
  A state is ergodic if it is persistent, non-null and aperiodic. A chain is ergodic if all it's states are ergodic. 
\end{definition}

Ergodism is a class property, since periodicity and persistency are both class properties.

\begin{theorem}
  An irreducible Markov chain always has a limiting distribution. 
\end{theorem}

\begin{exercise}
  Consider a gambler who at each play of the game has probability $p$ of winning one unit and probability $q=1-p$ of losing one unit. Assuming that successive plays of the game are independent , what is the probability that starting with $i$ units, the gambler's fortune will reach $N$ before reaching 0?  
\end{exercise}
\begin{solution}
  Let $\{X_n,n=\{0,1,2,...\}\}$ be a Markov chain, where $X_n$ is the player's fortune at time $n$. Then, the transition probability $P$ is given by:
  \[P_{00} = P_{NN} = 1\]
  \[P_{i,i+1} = p = 1-P_{i,i-1}, i = 1,2,...,N-1\]
  As such, our Markov chain has 3 classes - $\{0\}, \{1,2,3,...,N-1\}, \{N\}$. The second class is transient, the others are recurrent. Hence, at some point the gambler will go broke or hit the jackpot of $N$.
  If $P_i$ is the probability of reaching $N$ from $i$ eventually, then:
  \[P_i = pP_{i+1} + qP_{i-1}, i = 1,2,...,N-1\]
  \[pP_i + qP_i = pP_{i+1} + qP_{i-1}\]
  \[P_{i+1} - P_i = \frac{q}{p}(P_i - P_{i-1})\]
  Of course, $P_0 = 0, P_N = 1$. From our recurrence, 
  \[P_i - P_{i-1} = \left( \frac{q}{p} \right)^{i-1} P_1\]
  Adding these equations over all $i$,
  \[P_i - P_1 = P_1 \left[\left(\frac{q}{p}\right) + ... + \left(\frac{q}{p} \right)^{i-1}  \right]\]
  Hence, 
  \[P_i = \begin{cases}
    \frac{1-(q/p)^i}{1-(q/p)}P_1 & \frac{q}{p} \neq 1 \\
    iP_1 & \text{if } \frac{q}{p} = 1
  \end{cases}\]
  Using the fact that $P_N = 1$,
  \[P_1 = \begin{cases}
    \frac{1-(q/p)}{1-(q/p)^N} & \frac{q}{p} \neq 1 \\
    \frac{1}{N} & \text{if } \frac{q}{p} = 1
  \end{cases}\]
  Hence,
  \[P_i = \begin{cases}
    \frac{1-(q/p)^i}{1-(q/p)^N} & p \neq \frac{1}{2} \\
    \frac{1}{N} & p = \frac{1}{2}
  \end{cases}\]
  We can see that if $p \leq \frac{1}{2}$, the gambler will definitely go broke against an infinitely rich adversary. Otherwise, there is a positive probability of increasing indefinitely.
\end{solution}

\chapter{Poisson Processes}

\section{Memorylessness}

\begin{definition}
Let $X$ be a continuous random variable whose values lie in the non negative real numbers. The probability distribution of $X$ is \textbf{memoryless} if for any non negative real numbers $t$ and $s$, we have:
\[P(X>t + s | X > t) = P(X>s)\]
\end{definition}

Hence, the distribution of a ``waiting time'' until a certain event does not depend on how much time has elapsed already.

\begin{theorem}
  A positive continuous random variable $X$ is memoryless if and only if $X \sim Exp(x;\lambda)$ for some $\lambda$.
\end{theorem}

\section{Counting Process}

\begin{definition}
A stochastic process $\{N(t),t \geq 0\}$ is said to be a counting process if $N(t)$ represents the total number of events that occur by time $t$.  
\end{definition}

From it's definition, 
\begin{itemize}
  \item $N(t) \geq 0$
  \item $N(t)$ is integer valued
  \item If $s < t$ then $N(s) \leq N(t)$
  \item For $s < t$, $N(t)-N(s)$ equals the number of events that occur in the interval $(s,t]$
\end{itemize}

A counting process has \textbf{independent increments}  if the numbers of events that occur in disjoint time intervals are independent. A counting process has \textbf{stationary increments} if the distribution of the number of events that occur in any time interval only depends on the length of that time interval.

\section{Poisson Process}

\begin{definition}
  The counting process $\{N(t),t\}$ is said to be a Poisson Process having rate $\lambda, \lambda > 0$ if 
  \begin{enumerate}
    \item $N(0) = 0$
    \item The process has independent increments
    \item The number of events in any interval of length $t$ is Poisson distributed with mean $\lambda t$. That is, for all $s,t \geq 0$
      \[P(N(t+s) - N(s) = n) = e^{- \lambda t}\frac{(\lambda t) ^ n}{n!}\]
      This means it has stationary increments.
  \end{enumerate}
\end{definition}

The definition of a Poisson process can also be encoded in the form of three postulates, which must be present for a counting process to be a Poisson process:

\begin{enumerate}
  \item \textbf{Independence} : $N(t+h) - N(t)$ is independent of the number of occurrences prior to that interval
  \item \textbf{Homogeneity in time} : $p_n(t)$ depends only on the length $t$ of the interval and is independent of where the interval is situated, where:
    \[p_n(t) = P(N(t+s) - N(s) = n)\]
  \item \textbf{Regularity} : In an interval of infinitesimal length $h$, the probability of exactly one occurrence $\lambda h + o(h)$ and that of more than one occurrence is $o(h)$
\end{enumerate}

We can prove that these definitions are equivalent.

\begin{theorem}
  The probability distribution of a Poisson Process is:
  \[p_n(t) = e^{\lambda t} \frac{(\lambda t)^n}{n!}\]
\end{theorem}
\begin{proof}
  Consider $p_n(t+h)$ for $n \geq 0$. $n$ events by epoch $t+h$. $n$ events by epoch $t+h$ can happen in the following ways:
  \begin{itemize}
    \item $n$ events by epoch $t$ and no event between $t$ and $t+h$. So,
      \begin{align*}
        P(A_1) &= P(N(t) = n) P(N(h) = 0 | N(t) = n) \\
               &= p_n(t) p_0(h)\\
               &= p_n(t)(1-\lambda h) + o(h)
      \end{align*}
    \item  $n-1$ events by epoch $t$ and one event between $t$ and $t+h$.
      \[P(A_2) = p_{n-1}(t) (\lambda h) + o(h)\]
    \item $n-2$ events by epoch $t$ and two events between $t$ and $t+h$.
      \[P(A_3) = p_{n-2}(t) (p_2(h)) \leq p_2(h)\]
      and so on for $A_4, A_5, ...$
      \[\sum_{k=2}^n P(A_{k+1}) \leq \sum_{k=2}^n p_k(h) = o(h)\]
  \end{itemize}
  and so
      \[p_n(t+h) = p_n(t) (1- \lambda h) + p_{n-1}(t) (\lambda h) + o(h), n \geq 1\]
      Setting the limit $h \rightarrow 0$
      \[p_n'(t) = -\lambda (p_n(t) - p_{n-1}(t))\]
      If $n =0$, then we get:
      \[p_0'(t) = -\lambda(p_0(t))\]
      Obviously, $p_0(0) = 1$ and $p_n(0) = 0, n > 0$.
  With this, we can prove our result using induction or generating functions. Using generating functions,
  \[P(s,t) = \sum_{n=0}^{\infty} p_n(t)s^n\]
  TODO
\end{proof}

\section{Interarrival times}

Let $Z_i$ be the time between two occurrences $t_{i-1}$ and $t_i$. Then $Z_i$ is called the \textbf{inter-arrival time}.

\begin{theorem}
  $Z_i$ in a Poisson process (with mean $\lambda t$) are identically independently distributed random variables which follow the negative exponential distribution with mean $1/\lambda$.
\end{theorem}

The proof of this is beyond the scope of the course. In fact, the converse also holds - if the inter-arrival times follow the negative exponential distribution, then the process must be a Poisson process.

Since inter-arrival times follow an exponential distribution, it also follows that they are memoryless.

\begin{lemma}
  If $Z_1, \cdots, Z_n$ are i.i.d with $Z_i \sim Exp(z;\lambda)$ then $S_n = \sum_{i = 1}^{n}  Z_i$ is distributed according to a Gamma distribution with $n$ and $\lambda$, i.e.,
  \[f_X(t;n,\lambda) = \frac{\lambda^n}{\Gamma (\alpha)} t^{n-1} e^{-\lambda t}\]
\end{lemma}

The condition for the above theorem to apply is always going to be present when considering Poisson processes

\section{Properties of a Poisson Process}

A Poisson process has the following properties:

\begin{enumerate}
  \item For a Poisson process with rate $\lambda$, 
    \[E[N(t)] = Var(N(t)) = \lambda t\]
  \item The sum of two independent Poisson processes is a Poisson process
  \item A random selection from a Poisson process yields a Poisson Process. Suppose that $N(t)$, the number of occurrences of an event $E$ in an interval length $t$ is a Poisson process with parameter $\lambda$. Suppose also that each occurrence of $E$ has a constant probability $p$ of being recorded and that that the recording of an occurrence is independent of that of other occurrences and also of $N(t)$. If $M(t)$ is the number of occurrences recorded in an interval of length $t$, then $M(t)$ is also a Poisson process with parameter $\lambda p$
\end{enumerate}

\section{Poisson Cluster Process}

Let us suppose that several events can happen simultaneously at an instance, i.e. we have a cluster of occurrences at a point. We assume that:
\begin{enumerate}
  \item The number $N(t)$ of clusters in time $t$ constitute a Poisson process with mean rate $\lambda$
  \item Each cluster has a random number of occurrences, i.e. the number $X_i$ of occurrences in the $i^{th}$ cluster is a random variable. The number of occurrences in the different clusters are independent and follow the same probability distribution $Pr(X_i = k) = p_k$, having probability generating function:
    \[P(s) = \sum_{k = 1}^{\infty} p_ks^k\]
\end{enumerate}

\begin{theorem}
  If $M(t)$ denotes the total number of occurrences in an interval of length $t$ under the given conditions, then the generating function is given by:
  \[G(P(s)) = \exp [\lambda t \{P(s)-1\}]\]
\end{theorem}
\begin{proof}
  To prove this, we use the lemma that the sum $S_n = X_1 + \cdots + X_n$ of a fixed number $n$ of identically and independently distributed random variables $X_i$ has the probability generating function $\{P(s)\}^n$, each $X_i$ having probability generating function $P(s)$.
\end{proof}

\section{Non-homogeneous Poisson Processes}

In a non-homogeneous Poisson process, the rate $\lambda$ varies with time, i.e. $\lambda(t)$ is a function of $t$. $P_n(t)$ is the probability that $n$ events occur by time $t$.

\begin{theorem}
  \[P_n(t) = \frac{1}{n!} (m(t))^k e^{-m(t)}\] 
  where
  \[m(t) = \int_0^t \lambda(x) dx\]
\end{theorem}

\section{Yule-Furry Process}

Here we consider that $\lambda$ is a function of $n$, the population size at an instant. We assume that:

\begin{align*}
  p_k(h) = Pr(N(t+h) - N(t) = k | N(t) = n) &= \lambda_n h + o(h), k = 1\\
                                            &= o(h), k \geq 2\\
                                            &= 1 - \lambda_nh + o(h), k = 0
\end{align*}

where $\lambda_n = n \lambda$ and the initial condition is $p_1(0), p_i(0) = 0$ for $i \neq 1$. The general process where $\lambda$ is a function of $n$ is called a pure birth process, but if $\lambda_n = n \lambda$, then it is a Yule-Furry Process.

An example of such a process is a population of bacteria that can give birth to new members but cannot die. 

\begin{theorem}
  A Yule-Furry Process is given by the following characteristics
  \[p_n(t) = e^{-\lambda t} (1 - e^{-\lambda t})^{n-1}\] 
  \[E[N(t)] = e^{\lambda t}\] 
  \[Var(N(t)) = e^{\lambda t} (e^{\lambda t} - 1)\]
\end{theorem}

Let us suppose that there is also immigration in addition to birth, such that in an interval of infinitesimal length $h$, the probability of a new member being added (by immigration) is $vh + oh$.

Then the generating function of the probability is:
\[P(s,t) = \frac{s^i e^{-\lambda t} e^{-v(1-s)t}}{(1 - s(1- e^{-\lambda t}))^i}\]

\end{document}
